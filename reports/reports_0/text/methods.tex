\section{Methods}
\subsubsection{Data Collection}
Hospitalizations with a discharge date from January 2011 to May 2018 were extracted %
from the Cleveland Clinic Foundation (CCF) electronic health record (EHR).\@
Clinical, demographic, and institutional features were extracted using natural language processing %
and parsing of structured data available within the EHR.\@
U.S. American Community Survey (ACS) demographic data was generated %
from census block group geographic identifiers (BlockGroup GeoIDs) using the %
publicly available application programming interface.\supercite{acs}\@
Full lists of variables used for each predictive goal are available in Tables S.XX.\@ 
%TODO: make tables S.XX

Readmission was defined as any new CCF hospitalization starting 4 hours after any CCF discharge, %
but before the predefined thresholds (3, 5, 7, and 30 days).\@
Length of stay was defined as the time between discharge and admission.\@
Death within 48--72 hours of admission was defined as a recorded EHR, Social Security, %
or Ohio Death Index death date, or a discharge disposition of ``expired,'' within the given time frame.\@
For prediction of readmission, patients whose discharge disposition was ``expired'' were removed.\@
Patients with an admission class of ``observation'' were retained, as it has been suggested that
readmission reduction programs have resulted in an increased use of the observational setting.\supercite{obs2016nejm, obsincreasing2019}
The 4 hour cutoff removed patients who were simply transferring from one CCF department or hospital %
to another, and was selected based on histogram analysis of first-day readmissions.\@ %~\ref{transferhisto}
For prediction of length of stay and death within 48--72 hours of admission, %
only variables available within roughly 24 hours of admission were considered, %
and patients with an admission class of ``observation'' were removed.\@

\sloppy
\input{pygen/methods_paragraphs_latex.txt}

\subsubsection{Predictive Modeling}
Gradient Boosting Decision Trees (GBDT), also known as 
Gradient Boosting Machines (GBM), were used to produce predictive models.\@
GBM is a nonparametric method that trains many decision trees in succession, using
information from each set of decision trees to optimize the performance
of the next iteration.\supercite{gbmtutorial} We used LightGBM v2.2.2,
which allows for rapid training by utilizing a histogram-based method
to discretize continuous variables, and achieves state-of-the-art performance in relation
to other machine learning methods.\supercite{lightgbm} It also allows for
inclusion of many types of variables, and can explicitly
account for missingness.\@ GBM models do not require feature scaling
or imputation of missing values.\@
To reduce model overfitting, we employed a standard train/test/validation split\supercite{esl}
and early stopping at 200 iterations.\supercite{zhang2005boosting}

\subsubsection{Model Explanations}
Global and personalized model predictions were interpreted using SHAP (SHapley Additive exPlanation) values.\supercite{lundberg2017unified}\@
SHAP values, based on the Shapley value from coalitional game theory, 
are consistent and accurate calculations of the contributions of each feature to any machine learning model's prediction.\@
SHAP values may be used to explain a model globally, by examining the average contribution of a given feature
to the model output, or locally, by examining the most important variables for a given prediction.\supercite{lundberg2018explainable} \@
They may also be used to examine interactions between variables.\@
SHAP values were generated using SHAP v0.28.5.\@
Visualizations were created using Matplotlib v3.0.3\supercite{matplotlib} and the \LaTeX\ typesetting language.\@


\subsubsection{Statistical Analysis}
Baseline characteristics were reported as median [quartiles 1 and 3] and frequencies (\%), as appropriate, on a per-hospitalization (rather than per-patient) basis.\@
Subgroup analyses based on 30-day readmissions and lengths of stay greater than 5 days were also performed using the same metrics.\@

Model performances were assessed with metrics appropriate to the prediction endpoint.\@ 
For binary outcomes %including readmission and length of stay, death within 48-72 hours of hospitalization, gender, race, and financial class,
the Brier score loss, area under the receiver operator characteristic curve (ROC AUC), and average precision were calculated.\@
Additionally, ROC curves, precision-recall curves, calibration plots, and confusion matrices were generated for visual assessments of model performance.\@
Lower Brier scores are better, with a score <0.25 generally considered indicative of a useful model.\supercite{Steyerberg2010}\@
They are calculated as the mean squared difference between the probabilty assigned to each sample and the actual outcome (1 or 0).\@
Calibration curves provide a visual representation of a similar concept, and show the model's predicted probability vs.\ 
the fraction of samples at that probability with the actual outcome.\@ 
The curve of perfectly calibrated model would exhibit a straight 45\degree\ line.\@
ROC curves, the corresponding ROC AUC, and precision-recall curves, with the average precision metric, show classification performance 
at all possible classification thresholds.\supercite{pencina2015evaluating} Higher numbers are better.\@
ROC AUC and average precision of 0.5 indicate a model that performs no better than chance.\@
An AUC of 1.0 indicates 100\% true positive and 0\% false positive rates, 
while an average precision of 1.0 indicates a positive predictive value of 100\%.\@
Confusion matrices show the number of samples correctly and incorrectly classified.\@
% TODO: 95%CI a la https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/
% or https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals 

For numeric outcomes including length of stay in days, days until readmission, and age, we calculated
root mean square error (RMSE), median absolute error, mean absolute error, and R\textsuperscript{2} scores.\@
RMSE is calculated as the square root of the average of squared errors, or difference between observed and expected values, 
and yields a metric in the same units as the predictive target (here, days or years).\supercite{chai2014root}\@
Median absolute error is the median absolute difference between the predicted target and the actual value,
and mean absolute error is the mean of the same.\@
R\textsuperscript{2} scores are the percentage of the target variable variation captured by the model, where 100\% indicates
a model that explains all of the variability and 0\% indicates a model that explains none.\@ %TODO: make residual plots for LoS! Like calibration curves for regression.
Analyses were performed with Scikit-Learn v0.20.3\supercite{scikit-learn} and Python v3.6.6.\@
